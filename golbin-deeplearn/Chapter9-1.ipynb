{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist/data/train-images-idx3-ubyte.gz\n",
      "Extracting ./mnist/data/train-labels-idx1-ubyte.gz\n",
      "Extracting ./mnist/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./mnist/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('./mnist/data/', one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- learning_rate = 최적화 함수에서 사용할 학습률\n",
    "- training_epoch = 전체 데이터를 학습할 총횟수\n",
    "- batch_size = 미니배치로 한번에 학습할 데이터의 개수\n",
    "- n_hidden = 은닉층의 뉴런 개수\n",
    "- n_input = 입력값의 크기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_epoch = 100\n",
    "batch_size = 100\n",
    "learning_rate = 0.0002\n",
    "n_hidden = 256\n",
    "n_input = 28 * 28\n",
    "n_noise = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, [None, n_input])\n",
    "Z = tf.placeholder(tf.float32, [None, n_noise])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_W1 = tf.Variable(tf.random_normal([n_noise, n_hidden], stddev=0.01))\n",
    "G_b1 = tf.Variable(tf.zeros([n_hidden]))\n",
    "G_W2 = tf.Variable(tf.random_normal([n_hidden, n_input], stddev=0.01))\n",
    "G_b2 = tf.Variable(tf.zeros([n_input]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_W1 = tf.Variable(tf.random_normal([n_input, n_hidden], stddev=0.01))\n",
    "D_b1 = tf.Variable(tf.zeros([n_hidden]))\n",
    "D_W2 = tf.Variable(tf.random_normal([n_hidden, 1], stddev=0.01))\n",
    "D_b2 = tf.Variable(tf.zeros([1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(noise_z):\n",
    "    hidden = tf.nn.relu(tf.matmul(noise_z, G_W1) + G_b1)\n",
    "    output = tf.nn.sigmoid(tf.matmul(hidden, G_W2) + G_b2)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator(inputs):\n",
    "    hidden = tf.nn.relu(tf.matmul(inputs, D_W1) + D_b1)\n",
    "    output = tf.nn.sigmoid(tf.matmul(hidden, D_W2) + D_b2)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_noise(batch_size, n_noise):\n",
    "    return np.random.normal(size=(batch_size, n_noise))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = generator(Z)\n",
    "D_gene = discriminator(G)\n",
    "D_real = discriminator(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_D = tf.reduce_mean(tf.log(D_real) + tf.log(1 - D_gene))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_G = tf.reduce_mean(tf.log(D_gene))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_var_list = [D_W1, D_b1, D_W2, D_b2]\n",
    "G_var_list = [G_W1, G_b1, G_W2, G_b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_D = tf.train.AdamOptimizer(learning_rate).minimize(-loss_D, var_list=D_var_list)\n",
    "train_G = tf.train.AdamOptimizer(learning_rate).minimize(-loss_G, var_list=G_var_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:    0 D loss: -0.4606 G loss: -2.149\n",
      "Epoch:    1 D loss: -0.2708 G loss: -2.535\n",
      "Epoch:    2 D loss: -0.09213 G loss: -3.532\n",
      "Epoch:    3 D loss: -0.4732 G loss: -1.537\n",
      "Epoch:    4 D loss: -0.3575 G loss: -1.86\n",
      "Epoch:    5 D loss: -0.4892 G loss: -1.919\n",
      "Epoch:    6 D loss: -0.3289 G loss: -2.435\n",
      "Epoch:    7 D loss: -0.4689 G loss: -2.318\n",
      "Epoch:    8 D loss: -0.302 G loss: -2.575\n",
      "Epoch:    9 D loss: -0.4112 G loss: -2.195\n",
      "Epoch:   10 D loss: -0.321 G loss: -2.703\n",
      "Epoch:   11 D loss: -0.4394 G loss: -2.368\n",
      "Epoch:   12 D loss: -0.3339 G loss: -2.425\n",
      "Epoch:   13 D loss: -0.516 G loss: -2.034\n",
      "Epoch:   14 D loss: -0.4579 G loss: -2.446\n",
      "Epoch:   15 D loss: -0.6029 G loss: -1.922\n",
      "Epoch:   16 D loss: -0.4698 G loss: -2.162\n",
      "Epoch:   17 D loss: -0.3778 G loss: -2.167\n",
      "Epoch:   18 D loss: -0.4896 G loss: -2.136\n",
      "Epoch:   19 D loss: -0.5377 G loss: -1.899\n",
      "Epoch:   20 D loss: -0.5769 G loss: -1.849\n",
      "Epoch:   21 D loss: -0.5748 G loss: -1.931\n",
      "Epoch:   22 D loss: -0.5777 G loss: -2.312\n",
      "Epoch:   23 D loss: -0.677 G loss: -1.858\n",
      "Epoch:   24 D loss: -0.5749 G loss: -2.237\n",
      "Epoch:   25 D loss: -0.2969 G loss: -2.482\n",
      "Epoch:   26 D loss: -0.4381 G loss: -2.399\n",
      "Epoch:   27 D loss: -0.7221 G loss: -1.971\n",
      "Epoch:   28 D loss: -0.5429 G loss: -2.364\n",
      "Epoch:   29 D loss: -0.6406 G loss: -2.332\n",
      "Epoch:   30 D loss: -0.6429 G loss: -2.276\n",
      "Epoch:   31 D loss: -0.6297 G loss: -2.25\n",
      "Epoch:   32 D loss: -0.7008 G loss: -1.974\n",
      "Epoch:   33 D loss: -0.7936 G loss: -1.985\n",
      "Epoch:   34 D loss: -0.5474 G loss: -2.12\n",
      "Epoch:   35 D loss: -0.5821 G loss: -2.274\n",
      "Epoch:   36 D loss: -0.4872 G loss: -2.399\n",
      "Epoch:   37 D loss: -0.5883 G loss: -2.204\n",
      "Epoch:   38 D loss: -0.6953 G loss: -1.777\n",
      "Epoch:   39 D loss: -0.4557 G loss: -2.416\n",
      "Epoch:   40 D loss: -0.6877 G loss: -2.384\n",
      "Epoch:   41 D loss: -0.7401 G loss: -2.164\n",
      "Epoch:   42 D loss: -0.8507 G loss: -1.987\n",
      "Epoch:   43 D loss: -0.709 G loss: -1.941\n",
      "Epoch:   44 D loss: -0.7656 G loss: -1.902\n",
      "Epoch:   45 D loss: -0.6844 G loss: -2.01\n",
      "Epoch:   46 D loss: -0.712 G loss: -2.076\n",
      "Epoch:   47 D loss: -0.6525 G loss: -1.938\n",
      "Epoch:   48 D loss: -0.8182 G loss: -1.761\n",
      "Epoch:   49 D loss: -0.7455 G loss: -1.841\n",
      "Epoch:   50 D loss: -0.7831 G loss: -1.715\n",
      "Epoch:   51 D loss: -0.8591 G loss: -1.977\n",
      "Epoch:   52 D loss: -0.936 G loss: -1.995\n",
      "Epoch:   53 D loss: -0.8101 G loss: -1.821\n",
      "Epoch:   54 D loss: -0.7765 G loss: -1.761\n",
      "Epoch:   55 D loss: -0.9188 G loss: -1.751\n",
      "Epoch:   56 D loss: -0.8326 G loss: -1.769\n",
      "Epoch:   57 D loss: -0.8423 G loss: -1.834\n",
      "Epoch:   58 D loss: -0.8226 G loss: -1.744\n",
      "Epoch:   59 D loss: -0.9939 G loss: -1.556\n",
      "Epoch:   60 D loss: -0.9646 G loss: -1.525\n",
      "Epoch:   61 D loss: -0.8697 G loss: -1.74\n",
      "Epoch:   62 D loss: -0.8441 G loss: -1.766\n",
      "Epoch:   63 D loss: -0.9372 G loss: -1.578\n",
      "Epoch:   64 D loss: -0.9591 G loss: -1.684\n",
      "Epoch:   65 D loss: -0.8607 G loss: -1.598\n",
      "Epoch:   66 D loss: -0.9379 G loss: -1.57\n",
      "Epoch:   67 D loss: -0.7619 G loss: -1.698\n",
      "Epoch:   68 D loss: -0.9694 G loss: -1.536\n",
      "Epoch:   69 D loss: -1.011 G loss: -1.547\n",
      "Epoch:   70 D loss: -0.8311 G loss: -1.68\n",
      "Epoch:   71 D loss: -0.8524 G loss: -1.562\n",
      "Epoch:   72 D loss: -0.8985 G loss: -1.625\n",
      "Epoch:   73 D loss: -0.72 G loss: -1.712\n",
      "Epoch:   74 D loss: -0.8865 G loss: -1.52\n",
      "Epoch:   75 D loss: -0.9531 G loss: -1.647\n",
      "Epoch:   76 D loss: -0.9422 G loss: -1.6\n",
      "Epoch:   77 D loss: -0.9125 G loss: -1.654\n",
      "Epoch:   78 D loss: -0.816 G loss: -1.865\n",
      "Epoch:   79 D loss: -0.7515 G loss: -1.647\n",
      "Epoch:   80 D loss: -0.8897 G loss: -1.561\n",
      "Epoch:   81 D loss: -0.8666 G loss: -1.657\n",
      "Epoch:   82 D loss: -0.9467 G loss: -1.477\n",
      "Epoch:   83 D loss: -0.9058 G loss: -1.736\n",
      "Epoch:   84 D loss: -0.8758 G loss: -1.719\n",
      "Epoch:   85 D loss: -0.879 G loss: -1.573\n",
      "Epoch:   86 D loss: -1.002 G loss: -1.48\n",
      "Epoch:   87 D loss: -0.8281 G loss: -1.674\n",
      "Epoch:   88 D loss: -0.769 G loss: -1.637\n",
      "Epoch:   89 D loss: -0.8027 G loss: -1.74\n",
      "Epoch:   90 D loss: -0.8136 G loss: -1.665\n",
      "Epoch:   91 D loss: -0.8787 G loss: -1.653\n",
      "Epoch:   92 D loss: -0.863 G loss: -1.635\n",
      "Epoch:   93 D loss: -0.9245 G loss: -1.779\n",
      "Epoch:   94 D loss: -0.804 G loss: -1.698\n",
      "Epoch:   95 D loss: -0.9242 G loss: -1.601\n",
      "Epoch:   96 D loss: -0.88 G loss: -1.788\n",
      "Epoch:   97 D loss: -0.691 G loss: -1.609\n",
      "Epoch:   98 D loss: -0.7499 G loss: -1.772\n",
      "Epoch:   99 D loss: -0.7707 G loss: -1.807\n",
      "finish!\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "for epoch in range(total_epoch):\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        noise = get_noise(batch_size, n_noise)\n",
    "        _, loss_val_D = sess.run([train_D, loss_D],\n",
    "                              feed_dict={X: batch_xs, Z: noise})\n",
    "        _, loss_val_G = sess.run([train_G, loss_G],\n",
    "                              feed_dict={Z: noise})\n",
    "        \n",
    "    print('Epoch:', '%4d' % epoch,\n",
    "         'D loss:', '{:.4}'.format(loss_val_D),\n",
    "         'G loss:', '{:.4}'.format(loss_val_G))\n",
    "    \n",
    "    if epoch == 0 or (epoch + 1) % 10 == 0:\n",
    "        sample_size = 10\n",
    "        noise = get_noise(sample_size, n_noise)\n",
    "        samples = sess.run(G, feed_dict={Z: noise})\n",
    "        \n",
    "        fig, ax = plt.subplots(1, sample_size, figsize=(sample_size, 1))\n",
    "        \n",
    "        for i in range(sample_size):\n",
    "            ax[i].set_axis_off()\n",
    "            ax[i].imshow(np.reshape(samples[i], (28, 28)))\n",
    "            \n",
    "        plt.savefig('samples/{}.png'.format(str(epoch).zfill(3)),\n",
    "                               bbox_inches='tight')\n",
    "        plt.close(fig)\n",
    "        \n",
    "print('finish!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

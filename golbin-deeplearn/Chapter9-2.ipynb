{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist/data/train-images-idx3-ubyte.gz\n",
      "Extracting ./mnist/data/train-labels-idx1-ubyte.gz\n",
      "Extracting ./mnist/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./mnist/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('./mnist/data/', one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- learning_rate = 최적화 함수에서 사용할 학습률\n",
    "- training_epoch = 전체 데이터를 학습할 총횟수\n",
    "- batch_size = 미니배치로 한번에 학습할 데이터의 개수\n",
    "- n_hidden = 은닉층의 뉴런 개수\n",
    "- n_input = 입력값의 크기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_epoch = 100\n",
    "batch_size = 100\n",
    "n_hidden = 256\n",
    "n_input = 28 * 28\n",
    "n_noise = 128\n",
    "n_class = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, [None, n_input])\n",
    "Y = tf.placeholder(tf.float32, [None, n_class])\n",
    "Z = tf.placeholder(tf.float32, [None, n_noise])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(noise, labels):\n",
    "    with tf.variable_scope('generator'):\n",
    "        inputs = tf.concat([noise, labels], 1)\n",
    "        \n",
    "        hidden = tf.layers.dense(inputs, n_hidden,\n",
    "                                activation=tf.nn.relu)\n",
    "        output = tf.layers.dense(hidden, n_input,\n",
    "                                activation=tf.nn.sigmoid)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator(inputs, labels, reuse=None):\n",
    "    with tf.variable_scope('discriminator') as scope:\n",
    "        if reuse:\n",
    "            scope.reuse_variables()\n",
    "            \n",
    "        inputs = tf.concat([inputs, labels], 1)\n",
    "        \n",
    "        hidden = tf.layers.dense(inputs, n_hidden,\n",
    "                                activation=tf.nn.relu)\n",
    "        output = tf.layers.dense(hidden, 1, \n",
    "                                activation=None)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_noise(batch_size, n_noise):\n",
    "    return np.random.uniform(-1., 1., size=[batch_size, n_noise])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = generator(Z, Y)\n",
    "D_real = discriminator(X, Y)\n",
    "D_gene = discriminator(G, Y, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_D_real = tf.reduce_mean(\n",
    "                tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                logits=D_real, labels=tf.ones_like(D_real)))\n",
    "\n",
    "loss_D_gene = tf.reduce_mean(\n",
    "                tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                logits=D_gene, labels=tf.zeros_like(D_gene)))\n",
    "\n",
    "loss_D = loss_D_real + loss_D_gene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_G = tf.reduce_mean(\n",
    "                    tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                        logits=D_gene, labels=tf.ones_like(D_gene)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_D = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n",
    "                              scope='discriminator')\n",
    "vars_G = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n",
    "                              scope='generator')\n",
    "\n",
    "train_D = tf.train.AdamOptimizer().minimize(loss_D, var_list=vars_D)\n",
    "train_G = tf.train.AdamOptimizer().minimize(loss_G, var_list=vars_G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:    0 D loss: 0.004585 G loss: 7.773\n",
      "Epoch:    1 D loss: 0.0308 G loss: 7.74\n",
      "Epoch:    2 D loss: 0.01284 G loss: 8.905\n",
      "Epoch:    3 D loss: 0.006581 G loss: 7.453\n",
      "Epoch:    4 D loss: 0.01334 G loss: 7.014\n",
      "Epoch:    5 D loss: 0.009981 G loss: 7.713\n",
      "Epoch:    6 D loss: 0.03764 G loss: 6.429\n",
      "Epoch:    7 D loss: 0.1532 G loss: 8.505\n",
      "Epoch:    8 D loss: 0.06401 G loss: 6.99\n",
      "Epoch:    9 D loss: 0.09657 G loss: 7.587\n",
      "Epoch:   10 D loss: 0.1595 G loss: 6.574\n",
      "Epoch:   11 D loss: 0.2381 G loss: 5.432\n",
      "Epoch:   12 D loss: 0.2345 G loss: 4.763\n",
      "Epoch:   13 D loss: 0.3427 G loss: 4.72\n",
      "Epoch:   14 D loss: 0.3134 G loss: 4.56\n",
      "Epoch:   15 D loss: 0.4904 G loss: 3.631\n",
      "Epoch:   16 D loss: 0.5014 G loss: 4.203\n",
      "Epoch:   17 D loss: 0.6498 G loss: 3.499\n",
      "Epoch:   18 D loss: 0.5144 G loss: 3.638\n",
      "Epoch:   19 D loss: 0.5789 G loss: 4.146\n",
      "Epoch:   20 D loss: 0.4627 G loss: 2.965\n",
      "Epoch:   21 D loss: 0.5027 G loss: 3.578\n",
      "Epoch:   22 D loss: 0.8223 G loss: 2.846\n",
      "Epoch:   23 D loss: 0.5248 G loss: 3.752\n",
      "Epoch:   24 D loss: 0.6047 G loss: 3.16\n",
      "Epoch:   25 D loss: 0.546 G loss: 2.52\n",
      "Epoch:   26 D loss: 0.717 G loss: 2.464\n",
      "Epoch:   27 D loss: 0.4858 G loss: 2.723\n",
      "Epoch:   28 D loss: 0.7348 G loss: 2.425\n",
      "Epoch:   29 D loss: 0.5836 G loss: 2.285\n",
      "Epoch:   30 D loss: 0.6851 G loss: 2.678\n",
      "Epoch:   31 D loss: 0.6584 G loss: 2.148\n",
      "Epoch:   32 D loss: 0.6249 G loss: 2.034\n",
      "Epoch:   33 D loss: 0.7966 G loss: 2.036\n",
      "Epoch:   34 D loss: 0.7324 G loss: 2.162\n",
      "Epoch:   35 D loss: 0.6922 G loss: 2.095\n",
      "Epoch:   36 D loss: 0.7369 G loss: 2.043\n",
      "Epoch:   37 D loss: 0.6066 G loss: 2.576\n",
      "Epoch:   38 D loss: 0.5696 G loss: 2.544\n",
      "Epoch:   39 D loss: 0.6 G loss: 2.339\n",
      "Epoch:   40 D loss: 0.8824 G loss: 2.14\n",
      "Epoch:   41 D loss: 0.7248 G loss: 2.164\n",
      "Epoch:   42 D loss: 0.6575 G loss: 3.031\n",
      "Epoch:   43 D loss: 0.6966 G loss: 2.245\n",
      "Epoch:   44 D loss: 0.6238 G loss: 2.556\n",
      "Epoch:   45 D loss: 0.6497 G loss: 2.601\n",
      "Epoch:   46 D loss: 0.7444 G loss: 2.205\n",
      "Epoch:   47 D loss: 0.7195 G loss: 2.525\n",
      "Epoch:   48 D loss: 0.7227 G loss: 2.916\n",
      "Epoch:   49 D loss: 0.6204 G loss: 2.483\n",
      "Epoch:   50 D loss: 0.638 G loss: 2.259\n",
      "Epoch:   51 D loss: 0.8597 G loss: 1.874\n",
      "Epoch:   52 D loss: 0.6389 G loss: 2.475\n",
      "Epoch:   53 D loss: 0.7349 G loss: 2.282\n",
      "Epoch:   54 D loss: 0.7367 G loss: 1.946\n",
      "Epoch:   55 D loss: 0.7268 G loss: 1.933\n",
      "Epoch:   56 D loss: 0.8888 G loss: 2.202\n",
      "Epoch:   57 D loss: 0.6338 G loss: 2.598\n",
      "Epoch:   58 D loss: 0.7223 G loss: 2.324\n",
      "Epoch:   59 D loss: 0.66 G loss: 2.304\n",
      "Epoch:   60 D loss: 0.6442 G loss: 2.29\n",
      "Epoch:   61 D loss: 0.8596 G loss: 1.899\n",
      "Epoch:   62 D loss: 0.5622 G loss: 2.553\n",
      "Epoch:   63 D loss: 0.6185 G loss: 2.236\n",
      "Epoch:   64 D loss: 0.6689 G loss: 2.222\n",
      "Epoch:   65 D loss: 0.5909 G loss: 2.606\n",
      "Epoch:   66 D loss: 0.6977 G loss: 1.826\n",
      "Epoch:   67 D loss: 0.5893 G loss: 2.437\n",
      "Epoch:   68 D loss: 0.6507 G loss: 2.022\n",
      "Epoch:   69 D loss: 0.7552 G loss: 2.68\n",
      "Epoch:   70 D loss: 0.8324 G loss: 1.751\n",
      "Epoch:   71 D loss: 0.8531 G loss: 2.09\n",
      "Epoch:   72 D loss: 0.5724 G loss: 2.295\n",
      "Epoch:   73 D loss: 0.8143 G loss: 2.172\n",
      "Epoch:   74 D loss: 0.7157 G loss: 2.074\n",
      "Epoch:   75 D loss: 0.5316 G loss: 1.919\n",
      "Epoch:   76 D loss: 0.6917 G loss: 2.134\n",
      "Epoch:   77 D loss: 0.7041 G loss: 2.062\n",
      "Epoch:   78 D loss: 0.8231 G loss: 1.837\n",
      "Epoch:   79 D loss: 0.6004 G loss: 2.171\n",
      "Epoch:   80 D loss: 0.7367 G loss: 1.859\n",
      "Epoch:   81 D loss: 0.659 G loss: 2.351\n",
      "Epoch:   82 D loss: 0.7119 G loss: 2.294\n",
      "Epoch:   83 D loss: 0.6786 G loss: 1.92\n",
      "Epoch:   84 D loss: 0.6334 G loss: 1.967\n",
      "Epoch:   85 D loss: 0.5602 G loss: 2.617\n",
      "Epoch:   86 D loss: 0.7341 G loss: 1.995\n",
      "Epoch:   87 D loss: 0.7603 G loss: 2.006\n",
      "Epoch:   88 D loss: 0.6694 G loss: 2.397\n",
      "Epoch:   89 D loss: 0.6091 G loss: 2.379\n",
      "Epoch:   90 D loss: 0.7732 G loss: 2.505\n",
      "Epoch:   91 D loss: 0.8618 G loss: 2.709\n",
      "Epoch:   92 D loss: 0.5091 G loss: 2.539\n",
      "Epoch:   93 D loss: 0.7389 G loss: 1.985\n",
      "Epoch:   94 D loss: 0.6881 G loss: 2.115\n",
      "Epoch:   95 D loss: 0.6055 G loss: 2.158\n",
      "Epoch:   96 D loss: 0.6683 G loss: 2.207\n",
      "Epoch:   97 D loss: 0.6327 G loss: 2.589\n",
      "Epoch:   98 D loss: 0.7944 G loss: 2.136\n",
      "Epoch:   99 D loss: 0.6189 G loss: 1.948\n",
      "finish!\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "total_batch = int(mnist.train.num_examples / batch_size)\n",
    "loss_val_D, loss_val_G = 0, 0\n",
    "\n",
    "for epoch in range(total_epoch):\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        noise = get_noise(batch_size, n_noise)\n",
    "        _, loss_val_D = sess.run([train_D, loss_D],\n",
    "                              feed_dict={X: batch_xs, Y: batch_ys, Z: noise})\n",
    "        _, loss_val_G = sess.run([train_G, loss_G],\n",
    "                              feed_dict={Y:batch_ys, Z: noise})\n",
    "        \n",
    "    print('Epoch:', '%4d' % epoch,\n",
    "         'D loss:', '{:.4}'.format(loss_val_D),\n",
    "         'G loss:', '{:.4}'.format(loss_val_G))\n",
    "    \n",
    "    if epoch == 0 or (epoch + 1) % 10 == 0:\n",
    "        sample_size = 10\n",
    "        noise = get_noise(sample_size, n_noise)\n",
    "        samples = sess.run(G, feed_dict={Y: mnist.test.labels[:sample_size],\n",
    "                                         Z: noise})\n",
    "        \n",
    "        fig, ax = plt.subplots(2, sample_size, figsize=(sample_size, 2))\n",
    "        \n",
    "        for i in range(sample_size):\n",
    "            ax[0][i].set_axis_off()\n",
    "            ax[1][i].set_axis_off()\n",
    "            \n",
    "            ax[0][i].imshow(np.reshape(mnist.test.images[i], (28, 28)))\n",
    "            ax[1][i].imshow(np.reshape(samples[i], (28, 28)))\n",
    "        \n",
    "        plt.savefig('samples2/{}.png'.format(str(epoch).zfill(3)),\n",
    "                               bbox_inches='tight')\n",
    "        plt.close(fig)\n",
    "        \n",
    "print('finish!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

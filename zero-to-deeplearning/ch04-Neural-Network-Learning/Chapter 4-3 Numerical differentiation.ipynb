{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.3 수치 미분"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3.1 미분\n",
    "`\"특정 순간의 변화량\"`\n",
    "\n",
    "## $$\\frac{df(x)}{dx}= \\lim_{h->0}\\frac{f(x+h)-f(x)}{h}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_diff(f, x):\n",
    "    h = 1e-4 #0.0001\n",
    "    return (f(x+h) - f(x-h)) / (2*h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3.2 수치미분의 예\n",
    "### $ y = 0.01x^2 + 0.1x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_1(x):\n",
    "    return 0.01*x**2 + 0.1*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3d34078a58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(0.0, 20.0, 0.1)\n",
    "y = function_1(x)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"f(x)\")\n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3.3 편미분\n",
    "`변수가 여럿인 함수에 대한 미분`  \n",
    "## $\\frac{\\partial f}{\\partial x_0}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_tmp1(x0):\n",
    "    return x0*x0 + 4.0**2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 문제 1 : $x_0=3, x_1=4$일 때, $x_0$에 대한 편미분 $\\frac{\\partial f}{\\partial x_0}$ 를 구하라."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.00000000000378"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_diff(function_tmp1, 3.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 문제 2 : $x_0=3, x_1=4$일 때, $x_1$에 대한 편미분 $\\frac{\\partial f}{\\partial x_0}$ 를 구하라."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_tmp2(x1):\n",
    "    return 3.0**2.0 + x1*x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.999999999999119"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_diff(function_tmp2, 4.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 기울기 gradient\n",
    "`모든 변수의 편미분을 벡터로 정리한 것`  \n",
    "`기울기가 가리키는 쪽은 각 장소에서 함수의 출력 값을 가장 크게 줄이는 방향`\n",
    "## $(\\frac{\\partial f}{\\partial x_0},\\frac{\\partial f}{\\partial x_1})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4 #0.0001\n",
    "    grad = np.zeros_like(x) # x와 형상이 같은 배열을 생성\n",
    "    \n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "        \n",
    "        #f(x+h) 계산\n",
    "        x[idx] = tmp_val + h\n",
    "        fxh1 = f(x)\n",
    "        \n",
    "        #f(x-h) 계산\n",
    "        x[idx] = tmp_val - h\n",
    "        fxh2 = f(x)\n",
    "        \n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        x[idx] = tmp_val #값 복원\n",
    "        \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6.,  8.])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_gradient(function_2, np.array([3.0, 4.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  4.])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_gradient(function_2, np.array([0.0, 2.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6.,  0.])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_gradient(function_2, np.array([3.0, 0.0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4.1 경사하강법 gradient descent method\n",
    "`함수의 최솟값을 찾는 것`  \n",
    "## $x_0 = x_0-\\eta\\frac{\\partial f}{\\partial x_0}$\n",
    "## $x_1 = x_1-\\eta\\frac{\\partial f}{\\partial x_1}$\n",
    "\n",
    "## $\\eta$ = 갱신하는 값 = learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(f, init_x, lr=0.01, step_num=100):\n",
    "    x = init_x\n",
    "    \n",
    "    for i in range(step_num):\n",
    "        grad = numerical_gradient(f, x)\n",
    "        x -= lr * grad\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -6.11110793e-10,   8.14814391e-10])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_x = np.array([-3.0, 4.0])\n",
    "gradient_descent(function_2, init_x=init_x, lr=0.1, step_num=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from common.functions import softmax, cross_entropy_error\n",
    "from common.gradient import numerical_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class simpleNet:\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(2, 3)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        return np.dot(x, self.W)\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        z = self.predict(x)\n",
    "        y = softmax(z)\n",
    "        loss = cross_entropy_error(y, t)\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.1789094   0.25934593  0.47945994]\n",
      " [ 1.07606811 -0.06919851  0.37709532]]\n"
     ]
    }
   ],
   "source": [
    "net = simpleNet()\n",
    "print(net.W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.26111566  0.0933289   0.62706175]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([0.6, 0.9])\n",
    "p = net.predict(x)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.array([0, 0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82415442496389291"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.loss(x, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda w: net.loss(x, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "dw = numerical_gradient(f, net.W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.18251441,  0.15432216, -0.33683657],\n",
       "       [ 0.27377162,  0.23148324, -0.50525486]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from common.functions import *\n",
    "from common.gradient import numerical_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        #weight initialize\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * \\\n",
    "                            np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * \\\n",
    "                            np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "        \n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        \n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    # x : input data, t : goal lable\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        \n",
    "        return cross_entropy_error(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "    \n",
    "    # x: input data, t : goal lable\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 100)\n",
      "(100,)\n",
      "(100, 10)\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "net = TwoLayerNet(input_size=784, hidden_size=100, output_size=10)\n",
    "print(net.params['W1'].shape)\n",
    "print(net.params['b1'].shape)\n",
    "print(net.params['W2'].shape)\n",
    "print(net.params['b2'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.rand(100, 784)\n",
    "t = np.random.rand(100, 10)\n",
    "\n",
    "grads = net.numerical_gradient(x, t) # gradient calculate\n",
    "\n",
    "print(grads['W1'].shape)\n",
    "print(grads['b1'].shape)\n",
    "print(grads['W2'].shape)\n",
    "print(grads['b2'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "from dataset.mnist import load_mnist\n",
    "from two_layer_net import TwoLayerNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n"
     ]
    }
   ],
   "source": [
    "(x_train, t_train), (x_test, t_test) = \\\n",
    "    load_mnist(normalize=True, one_hot_label=True)\n",
    "    \n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "    \n",
    "#hyperparameter\n",
    "iters_num = 10000 # roop count\n",
    "train_size = x_train.shape[0]\n",
    "print(train_size)\n",
    "batch_size = 100 # minibatch size\n",
    "learning_rate = 0.1\n",
    "network = TwoLayerNet(input_size=784, hidden_size=100, output_size=10)\n",
    "\n",
    "#loop count of 1 epoch\n",
    "iter_per_epoch = max(train_size / batch_size, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc, test acc : 0.102183333333, 0.101\n",
      "train acc, test acc : 0.819583333333, 0.824\n",
      "train acc, test acc : 0.88245, 0.8872\n",
      "train acc, test acc : 0.900283333333, 0.9034\n",
      "train acc, test acc : 0.9085, 0.9103\n",
      "train acc, test acc : 0.913966666667, 0.9172\n",
      "train acc, test acc : 0.918033333333, 0.9212\n",
      "train acc, test acc : 0.923483333333, 0.9259\n",
      "train acc, test acc : 0.925983333333, 0.9268\n",
      "train acc, test acc : 0.929266666667, 0.9301\n",
      "train acc, test acc : 0.933033333333, 0.9329\n",
      "train acc, test acc : 0.935666666667, 0.9364\n",
      "train acc, test acc : 0.938583333333, 0.9384\n",
      "train acc, test acc : 0.939866666667, 0.9395\n",
      "train acc, test acc : 0.941633333333, 0.9418\n",
      "train acc, test acc : 0.944483333333, 0.9432\n",
      "train acc, test acc : 0.945666666667, 0.946\n"
     ]
    }
   ],
   "source": [
    "for i in range(iters_num):\n",
    "    # get minibatch\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    #calculate gradient\n",
    "    #grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    grad = network.gradient(x_batch, t_batch) #upgrade perfomance\n",
    "    \n",
    "    #update parameters\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "        \n",
    "    #write learning progress\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    #calculate accuracy to epoch of 1\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        print(\"train acc, test acc : \" + str(train_acc) + \", \" + str(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
